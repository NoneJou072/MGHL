# Q-Learning

Q 学习(Q-Learning)是一种**异策略（off-policy）** 算法。异策略在学习的过程中有两种不同的策略：

1. **目标策略（target policy）** : 目标策略是我们要去学习的策略，一般用 $\pi$ 来表示。目标策略可以根据自己的经验来学习最优的策略，不需要去和环境交互。目标策略 $\pi$ 优化的时候，只选取奖励最大的策略（贪心策略）, 取它下一步能得到的所有状态，不会管我们下一步去往哪里探索。即
$$\pi(s_{t+1})=\underset{a'}{argmax}\ Q(s_{t+1}, a')$$

2. **行为策略（behavior policy）** :行为策略是探索环境的策略，一般用 µ 来表示。行为策略可以在环境里探索到所有可能的轨迹并采集经验数据，然后把采集到的经验交给目标策略学习。行为策略 µ 可以是一个随机的策略，但我们采取 ε-贪心策略，让行为策略不至于是完全随机的，它是基于 Q 表格逐渐改进的。

Q-Learning 的伪代码如下所示：

* 初始化 $Q(s,a)$  
* for 序列 $e=1 \to E$ do：
    * 得到初始状态 $s$
    * for 时间步 $t=1\to T$ do :
        * 用$\epsilon$ -greedy 策略根据 Q 选择当前状态 s 下的动作 a
        * 得到环境反馈的 r, s'
        * $Q(s,a)\gets Q(s,a)+\alpha[r+\gamma \underset{a'}{max}Q(s',a')-Q(s,a)]$
        * $s\gets s'$
    * end for
* end for


异策略
---
在异策略学习的过程中，轨迹都是行为策略与环境交互产生的，产生这些轨迹后，我们使用这些轨迹来更新目标策略 $\pi$。
异策略学习有很多好处。首先，我们可以利用探索策略来学到最佳的策略，学习效率高；其次，
异策略学习可以让我们学习其他智能体的动作，进行模仿学习，学习人或者其他智能体产生的轨迹；
最后，异策略学习可以让我们重用旧的策略产生的轨迹，探索过程需要很多计算资源，这样可以节省资源。