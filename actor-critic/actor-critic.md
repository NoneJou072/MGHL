在 REINFORCE 算法中，每次需要根据一个策略采集一条完整的轨迹，并计算这条轨迹上的回报。这种采样方式的方差比较大，学习效率也比较低。我们可以借鉴时序差分学习的思想，使用时序差分方法来提高采样效率，即从状态 s 开始的总回报可以通过当前动作的即时奖励 r 和下一个状态 s′ 的值函数来近似估计。

## 演员-评论员算法
**演员-评论员算法（actor-critic）** 是一种结合策略梯度和时序差分学习的强化学习方法。借助于价值函数，演员-评论员算法可以进行单步参数更新，不需要等到回合结束才进行更新。
* **演员** 指策略网络 $π_θ(a|s)$，用于学习一个策略以得到尽可能高的回报。
* **评论员** 指价值网络 $V_π(s)$，用于对当前策略的值函数进行估计，即评估演员的好坏。

如图所示，演员负责根据环境目前的状态来输出一个动作。评论员会在每一个步骤估计演员的动作未来能有多少奖励，也就是估计演员输出的动作的 Q 值 $Q_\omega(s, π_θ(a\mid s))$。演员根据评论员的打分来调整自己的策略，也就是我们希望调整演员的网络参数 θ，使得评委打分尽可能高。评论员则要根据环境的反馈奖励来调整自己的打分策略，也就是要更新评论员的神经网络的参数 $\omega$ ，评论员的最终目标是让演员的表演获得观众尽可能多的欢呼声和掌声，从而最大化未来的总收益。

![image](../assets/ddpg2.png)

虽然价值网络 $q(s,a;\omega)$  与 DQN 结构相同，但两者意义不同，训练方法也不同：
* 价值网络是对动作价值函数 $Q_\pi(s,a)$ 的近似，而 DQN 是对最优动作价值函数 $Q_*(s,a)$ 的近似。
* 对价值网络的训练使用的是 SARSA 算法，它属于同策略，不能用经验回放。对 DQN 的训练使用的是 Q 学习算法，它属于异策略，可以用经验回放。