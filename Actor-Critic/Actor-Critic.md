通俗地解释一下演员-评论员结构。如图 10.6 所示，策略网络扮演的就是演员的角色，它负责根据环境目前的状态来输出一个动作。Q 网络就是评论员，它会在每一个步骤估计演员的动作未来能有多少奖励，也就是估计演员输出的动作的 Q 值，即 Q~w~(s, a)。演员根据评论员的打分来调整自己的策略，也就是更新演员的神经网络参数 θ。评论员则要根据环境的反馈奖励来调整自己的打分策略，也就是要更新评论员的神经网络的参数 w ，评论员的最终目标是让演员的表演获得观众尽可能多的欢呼声和掌声，从而最大化未来的总收益。

![image](../assets/ddpg2.png)


最开始训练的时候，这两个神经网络的参数是随机的。所以评论员最开始是随机打分的，演员也随机输出动作。但是由于有环境反馈的奖励存在，因此评论员的评分会越来越准确，所评判的演员的表现也会越来越好。既然演员是一个神经网络，是我们希望训练好的策略网络，我们就需要计算梯度来更新优化它里面的参数 θ 。简单来说，我们希望调整演员的网络参数，使得评委打分尽可能高。注意，这里的演员是不关注观众的，它只关注评委，它只迎合评委的打分 ​Q~w~(s, a)。