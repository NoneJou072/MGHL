### Hindsight Experience Replay
传统的强化学习算法都局限在单个目标上，训练好的算法只能完成这个目标，不能完成其他目标。当面对较复杂的复合任务时，需要训练多个智能体，每个智能体完成一个子任务，最后将这些子任务组合起来完成复合任务，这样会增加训练的复杂度。  
例如机械臂抓取任务，在使用传统的强化学习算法时，采用单一策略只能抓取同一个位置的物体。对于不同的目标位置，要训练多个策略。

另外这类环境通常还存在稀疏奖励问题，即只有当成功抓取目标后才能够获得奖励，在其他 steps 时失败的话将不会得到奖励。  

为了解决上面的问题，下面介绍 HER 方法。HER 的原理是回放每个具有任意目标的轨迹。当智能体在一个回合中没有达成目标时，可以使用另外的目标来替代原来的目标，之后根据这个新的目标重新计算奖励，来为智能体提供经验。HER改善了稀疏奖励 DRL 的采样效率，可以将该方法与任意的 off-policy 算法结合。

实现 HER 算法的关键是构建 replay buffer，包含以下两步：
1. 初始化一个固定长度的队列，每次进入一个五元组 $(s_t,a_t,r_t,s_{t+1},g)$，定义初始状态 $s_0$ 和目标 $g$, 智能体根据当前状态 $s_t$ 和目标 $g$ 来采取动作。
奖励的计算如下：
$$r_t \gets r(a_t,s_t||g)$$
采样出的 $(s_t,a_t,r_t,s_{t+1},g)$ 将被存放到 replay buffer 中。之后，每次都会基于实际目标 $g$ 采样一段完整的智能体经验序列。
2. 使用新的目标 $g'$ 重新计算奖励：
$$r' \gets r(a_t,s_t||g')$$
我们构建新的 transitions (st, at, r′ , st+1, g′) ，也存放到 replay buffer 中。

有四种方法去获得新的目标 $g'$:
* final: 将每个回合最后的状态作为新目标
* future: 随机选择 $k$ 个在这个轨迹上并且在当前transition之后的状态作为新目标
* episode: 每次选择 $k$ 个在这个轨迹上的状态作为新目标
* random: 每次在所有出现过的状态里面选择 $k$ 个状态作为新目标  

一般我们使用 future 方法，因为它能够更好地利用经验。
