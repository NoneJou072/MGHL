{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO-stable-baselines3 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C, PPO, SAC, TD3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from gym.wrappers import TimeLimit\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "\n",
    "def run_train():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env = TimeLimit(env, max_episode_steps=2000)\n",
    "    check_env(env)\n",
    "\n",
    "    # define callback function\n",
    "    class TensorboardCallback(BaseCallback):\n",
    "        \"\"\"\n",
    "        Custom callback for plotting additional values in tensorboard.\n",
    "        \"\"\"\n",
    "        def __init__(self, log_dir, verbose=0):\n",
    "            super(TensorboardCallback, self).__init__(verbose)\n",
    "            self.log_dir = log_dir\n",
    "\n",
    "        def _on_step(self) -> bool:\n",
    "            if self.n_calls % 51200 == 0:\n",
    "                print(\"Saving new best model\")\n",
    "                self.model.save(self.log_dir + f\"/model_saved/PPO/admit_diana_{self.n_calls}\")\n",
    "            return True\n",
    "\n",
    "    log_dir = \"log/\"\n",
    "\n",
    "    model = A2C(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        verbose=1,\n",
    "        tensorboard_log=log_dir,\n",
    "        device=\"cuda:0\",\n",
    "    )\n",
    "    # model = A2C.load(\"./log/model_saved/admit_diana_51200.zip\")\n",
    "    model.learn(total_timesteps=int(2e6), callback=TensorboardCallback(log_dir=log_dir))\n",
    "    # model.save(\"admit_diana\")\n",
    "    obs = env.reset()\n",
    "    ep_reward = 0\n",
    "    for i in range(int(1e6)):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        if i % 2000 == 0 or done:\n",
    "            env.reset()\n",
    "            print(ep_reward)\n",
    "            ep_reward = 0\n",
    "\n",
    "\n",
    "run_train()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
