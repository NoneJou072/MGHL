{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO(近端策略优化)\n",
    "## 1. 从同策略到异策略\n",
    "在强化学习里面，要学习的是一个智能体。如果要学习的智能体和与环境交互的智能体是相同的，称之为**同策略**。如果要学习的智能体和与环境交互的智能体不是相同的，称之为**异策略**。  \n",
    "在策略梯度中，演员与环境交互搜集数据，产生很多的轨迹 τ，根据搜集到的数据按照策略梯度的公式更新策略的参数，所以策略梯度是一个同策略的算法。PPO 是策略梯度的变形，它是现在 OpenAI 默认的强化学习算法。  \n",
    "$$\n",
    "\\bigtriangledown \\bar{R}_\\theta =\\mathbb{E}_{\\tau \\sim p_\\theta (\\tau )}[R(\\tau )\\bigtriangledown \\log p_\\theta (\\tau )] \n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作采样\n",
    "对于输入的状态，为了与神经网络模型的输入要求相匹配，我们需要在第0个维度上增加一个维度。\n",
    "在深度学习中，神经网络通常接受批量数据作为输入。这意味着输入数据通常具有批量维度，即一个维度用于表示输入数据的批量大小。例如，如果批量大小为32，则输入张量的形状将是(32, ...)\n",
    "\n",
    "在给定状态的情况下，我们希望将其作为一个批量大小为1的输入传递给神经网络模型。为了满足这个要求，我们需要在输入状态的维度上增加一个维度，使其形状变为(1, ...)，其中第0个维度表示批量大小为1。\n",
    "\n",
    "通过使用unsqueeze(dim=0)函数，我们可以在张量的第0个维度上增加一个维度，确保输入状态符合模型的输入要求。在代码中，它将状态张量的形状从(N, ...)（N是状态的维度）变为(1, N, ...)，将其转换为一个批量大小为1的张量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_action(self,state):\n",
    "        self.sample_count += 1\n",
    "        state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        self.log_probs = dist.log_prob(action).detach()\n",
    "        return action.detach().cpu().numpy().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
