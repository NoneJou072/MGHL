# TD3-双延迟深度确定性策略梯度

DDPG 对超参数和其他的调整经常很敏感。另外 DDPG 常见的问题是，已经学习好的 Q 函数开始高估 Q 值，导致策略被破坏，因为它利用了 Q函数中的误差。

**双延迟深度确定性策略梯度（Twin Delayed DDPG，TD3）** 通过引入 3 个关键技巧来解决这个问题：

* **截断双 Q 学习（clipped dobule Q-learning）**。TD3 在训练中需要学习两个 Q 函数 $Q_{\omega1}$ 、 $Q_{\omega2}$ 和一个策略函数。两个 Q 函数分别对应两个目标 Q 函数，取两个目标 Q 函数的较小值作为 TD 目标： 
$$y_i=r_i+\gamma \underset{i=1,2}{\min} Q_{\omega_i^-}(s', \mu_{\theta^-}(s'))$$

* **延迟策略更新（delayed policy updates）**。actor-critic 用价值网络指导策略网络的更新，如果价值网络本身不可靠，它给动作打分的结果就也是不可靠的。即在价值网络很差的时候更新动作网络，不但不会改进动作网络，还会由于动作网络的变化导致价值网络的训练不稳定。  
相关实验结果表明，应让 TD3 算法以较低的频率更新动作网络与三个目标网络。更好的方法是每一轮更新一次价值网络，每隔 k 轮更新一次策略网络和三个目标网络，k是超参数，通常不小于2。

* **在动作中加入噪声（目标策略平滑，target policy smoothing）**。TD3 以异策略的方式训练确定性策略。由于策略是确定性的，如果智能体要探索策略，则一开始它可能不会采样足够广泛的动作来找到有用的学习信号。为了使策略更好地探索，TD3 在目标动作中加入噪声，计算动作的过程变为
  $$ \hat{a}^-_{j+1}=\mu(s_{j+1};\theta^-)+\xi $$
  其中 $\xi$ 表示噪声，它是随机生成的向量，每一个元素独立随机从截断正态分布（clipped normal distribution）中抽取。把截断正态分布记作 $\mathcal{CN}(0,\sigma^2,-c,c)$，意思是均值为零、标准差为 $\sigma$ 的正态分布，且落在区间 $[-c,c]$ 之外的概率为零，以防止噪声过大。为了便于获取高质量的训练数据，我们可以在训练过程中减小噪声的大小。在测试时，为了查看策略对所学知识的利用程度，我们不会在动作中增加噪声。

Others:
* TD3 作者的[PyTorch 实现](https://github.com/sfujim/TD3/)
* 在 TD3 的论文中，TD3 的性能比**软演员-评论员（soft actor-critic，SAC）** 高。但在 SAC 的论文中，SAC 的性能比 TD3 高，这是因为强化学习的很多算法估计对参数和初始条件敏感。
